{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import codecs\n",
    "import string\n",
    "import gensim\n",
    "import unicodedata\n",
    "import copy as cp\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as pl\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "import sklearn.feature_selection as fs\n",
    "from time import time\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.cross_validation import KFold, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import vstack\n",
    "from itertools import cycle\n",
    "from sklearn.cluster import KMeans, AffinityPropagation, AgglomerativeClustering\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.decomposition import PCA, SparsePCA, TruncatedSVD, RandomizedPCA, NMF\n",
    "from sklearn.preprocessing import scale, Normalizer, Binarizer \n",
    "from sklearn.datasets.samples_generator import make_swiss_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_ne = \"juri-all-non-empty.csv\"\n",
    "all_e = \"juri.csv\"\n",
    "train2_8 = \"court_rulings_task2_8classes_train.csv\"\n",
    "test2_8 = \"court_rulings_task2_8classes_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('french')\n",
    "stop.append(u'dun')\n",
    "stop.append(u'dune')\n",
    "stop.append(u'les')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"french\")\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc)]\n",
    "\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
    "    return only_ascii\n",
    "\n",
    "\n",
    "def get_preprocessor(suffix=''):\n",
    "    def preprocess(unicode_text):\n",
    "        return unicode_text.strip().lower() + suffix\n",
    "    return preprocess\n",
    "\n",
    "def preprocess_data(X, n, suffix='', binarize=True):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, 1),\n",
    "                                 preprocessor=get_preprocessor(suffix), tokenizer=LemmaTokenizer())\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    X = Binarizer(copy=False).fit_transform(X) if binarize else X\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data_all(filename=all_e):\n",
    "    text, rule, area, date, claw = [], [], [], [], []\n",
    "    with codecs.open(filename, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            a = line.split(\"\\t\")\n",
    "            if (line != \"\") and (len(a)==7):\n",
    "                idx, loc, dec, dt, dsc, art, law  = line.split(\"\\t\") \n",
    "                if (loc != \"\") and (dsc != \"\") and (dec!=\"\") and (dt!=\"\") and (law!=\"\"):\n",
    "                    area.append(loc)\n",
    "                    rule.append(dec)\n",
    "                    d = str(int(dt.split(\"-\")[0])/10)\n",
    "                    date.append(d)\n",
    "                    text.append(dsc)\n",
    "                    claw.append(law)\n",
    "                \n",
    "    print len(rule), len(text)\n",
    "#     text = np.array(text)\n",
    "#     rule = np.array(rule)\n",
    "    \n",
    "#     rule = reduce_classes(rule)\n",
    "    return text, area, date, rule, claw\n",
    "\n",
    "def load_data(fname):\n",
    "    text, label = [], []\n",
    "    with codecs.open(fname, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            a = line.split(\"\\t\")\n",
    "            if (line != \"\") and (len(a)==2):\n",
    "                text.append(a[0])\n",
    "                label.append(a[1])\n",
    "#     text, label = np.array(text), np.array(label)\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xa, ya = load_data(\"court_rulings_task1_8classes_test.csv\")\n",
    "Xr, yr = load_data(\"court_rulings_task2_8classes_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_d2v(fin, fout):\n",
    "    print \"entered d2v training\"\n",
    "    sentences = doc2vec.TaggedLineDocument(fin)\n",
    "    model_court = gensim.models.Doc2Vec(sentences, size=200, workers =10, window=20)\n",
    "\n",
    "    model_court.save(fout)\n",
    "    \n",
    "def train_d2v2(fin, fout):\n",
    "    analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "    docs = []\n",
    "    with codecs.open(fin, encoding='utf-8') as f:\n",
    "        for line_no, line in enumerate(f):\n",
    "            doc = gensim.utils.to_unicode(line).split()\n",
    "            words = doc[:-1]\n",
    "            tags = [line_no]\n",
    "            docs.append(analyzedDocument(words, tags))\n",
    "    print \"len(docs)\", len(docs)\n",
    "            \n",
    "    model = doc2vec.Doc2Vec(docs, size = 200, window = 50, min_count = 1, workers =10)\n",
    "    model.save(fout)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_d2v2(\"court_rulings_task2_8classes_train.csv\", \"court-task2-8_train.d2v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_d2v2(\"court_rulings_task1_8classes_test.csv\", \"court-task1-8_test.d2v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = doc2vec.Doc2Vec.load(\"court-task1-8_test.d2v2\")\n",
    "model2 = doc2vec.Doc2Vec.load(\"court-task2-8_test.d2v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(words, n, count=True, reduced=True, n_labels=8):\n",
    "    vectorizer = CountVectorizer(analyzer='word', ngram_range=(1, n), binary=count, preprocessor=get_preprocessor())\n",
    "    transformed_words = vectorizer.fit_transform(words)\n",
    "#     transformed_words = np.array(transformed_words, dtype=np.float)\n",
    "    \n",
    "    if reduced:\n",
    "        svd = TruncatedSVD(n_labels)\n",
    "        normalizer = Normalizer(copy=False)\n",
    "        lsa = make_pipeline(svd, normalizer)\n",
    "        reduced_X = lsa.fit_transform(transformed_words)\n",
    "        return reduced_X, svd\n",
    "    else:\n",
    "        return transformed_words\n",
    "    \n",
    "def get_docvecs(vecs):\n",
    "    np_vecs = np.zeros((len(vecs), len(vecs[0])), dtype=vecs[0].dtype)\n",
    "    for i in range(len(vecs)):\n",
    "        np_vecs[i] = vecs[i]\n",
    "    return np_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affinity(X, labels, extract=False):\n",
    "    if extract == True:\n",
    "        print \"Extracting features...\"\n",
    "        X, _ = extract_features(articles, 1, False)\n",
    "    X_norms = np.sum(X * X, axis=1)\n",
    "    S = -X_norms[:, np.newaxis] - X_norms[np.newaxis, :] + 2 * np.dot(X, X.T)\n",
    "    p = 10 * np.median(S)\n",
    "\n",
    "    print \"Fitting affinity propagation clustering with unknown no of clusters...\"\n",
    "    af = AffinityPropagation().fit(S, p)\n",
    "    indices = af.cluster_centers_indices_\n",
    "#     for i, idx in enumerate(indices):\n",
    "#         print i, articles[idx].encode(\"utf8\")\n",
    "\n",
    "    n_clusters_ = len(indices)\n",
    "\n",
    "    print \"Fitting PCA...\"\n",
    "    X = RandomizedPCA(2).fit(X).transform(X)    \n",
    "    \n",
    "    print \"Plotting...\"\n",
    "    pl.figure(1)\n",
    "    pl.clf()\n",
    "    \n",
    "    colors = cycle('bgrcmyk')\n",
    "    for k, col in zip(range(n_clusters_), colors):\n",
    "        class_members = af.labels_ == k\n",
    "        cluster_center = X[indices[k]]\n",
    "        pl.plot(X[class_members,0], X[class_members,1], col+'.')\n",
    "        pl.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "                                         markeredgecolor='k', markersize=14)\n",
    "        for x in X[class_members]:\n",
    "            pl.plot([cluster_center[0], x[0]], [cluster_center[1], x[1]], col) \n",
    "\n",
    "    pl.title('Estimated number of clusters: %d' % n_clusters_)\n",
    "    pl.show()\n",
    "    pl.savefig(\"affinity_cluster.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_new, _ = extract_features(X, 1, False, True, 8)\n",
    "# affinity(X_new, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_docvecs = get_docvecs(model.docvecs)\n",
    "# affinity(np_docvecs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63214, 61362)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.docvecs), len(model2.docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gensim.models.doc2vec.DocvecsArray, numpy.ndarray)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.docvecs), type(np_docvecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Xa[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maxim = 0\n",
    "# for i in range(model.docvecs.count):\n",
    "#     aux = model.docvecs.most_similar(i, topn=1)[0][1]\n",
    "#     if maxim < aux:\n",
    "#         maxim = aux\n",
    "# maxim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = model.docvecs.most_similar(30, topn=20)\n",
    "r = model.docvecs.most_similar(30, topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "print type(model.docvecs[0])\n",
    "for i in range(len(model.docvecs)):\n",
    "    a.append(model.docvecs[i])\n",
    "    \n",
    "print type("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(gensim.models.doc2vec.DocvecsArray, numpy.ndarray)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.docvecs), type(model.docvecs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label of target document CHAMBRE_CIVILE_1\n",
      "\n",
      "-----------------\n",
      "label of top n most similar documents:\n",
      "-----------------\n",
      "CHAMBRE_CIVILE_3\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_CIVILE_3\n",
      "\n",
      "CHAMBRE_COMMERCIALE\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_CIVILE_3\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_COMMERCIALE\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_CIVILE_3\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_CIVILE_2\n",
      "\n",
      "CHAMBRE_CIVILE_1\n",
      "\n",
      "CHAMBRE_SOCIALE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"label of target document\", ya[30]\n",
    "print \"-----------------\"\n",
    "print \"label of top n most similar documents:\"\n",
    "print \"-----------------\"\n",
    "for i in a:\n",
    "    print ya[i[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-6fa7b1c1f2a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "skf = StratifiedKFold(y, n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
